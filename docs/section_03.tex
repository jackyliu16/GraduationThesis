\section{系统开发流程}

    按照现有嵌入式企业的嵌入式软件开发流程，开发一个嵌入式系统主要分为以下几个步骤。

\subsection{用户需求分析}

    根据现有的各种企事业单位对于考勤打卡的需求，需求的特性主要为特征性识别以及日志记录。
    
    因此，在总体设计上我计划采用最简单的由指纹识别模块获取输入，经过 MCU 中简单处理再转发给
    linux 下的控制主机的设计。

\subsection{嵌入式开发环境的搭建与说明}

    由于嵌入式开发所基于的 MCU 一般性能相当有限，就算是采用一般 linux 操作系统进行本机编译，其占用时间也会相对比较长，同时，也无法应对一些占用系统内存资源较大的编译场景。
    因此，在嵌入式开发中，一般通过交叉编译的方式实现在 x86\_64-linux 平台或其他通用操作系统架构平台上实现对于目标平台代码的编译以更好的利用硬件资源。

    在本文的实现过程中基于 \href{https://nixos.org/}{Nix} 管理 x86\_64-linux 平台实现了对 aarch64-unknown-linux 目标平台的编译，其中主要的编译工具链的部分直接采用原有 \href{https://github.com/rcore-os/arceos}{ArceOS} 操作系统实现的 Rust 语言交叉编译以及镜像处理步骤，将 Cargo 包管理工具直接生成的裸机 elf 文件通过 rust-objcopy 去除其中一些无关的信息，如调试信息等内容成为一个纯粹的二进制文件。 
  
    一般树莓派的流程由 保存在 Soc ROM 区中的 booloader 完成 SD 卡上 FAT32 分区的挂载以及加载第二阶段 bootcode.bin，但是由于我使用的树莓派4B (bcm2711) 相对于前代有不少的硬件更新，在我使用的树莓派中对应初始化，启用 GPU fireware 加载 start.elf 的 bootloader 代码都被实现在 EEPROM 中，以提升 ROM 代码的容错性。 
    在运行 start4.elf 文件时，其会对 sd 卡中的 config.txt 文件进行解析，完成对应如串口传输频率，是否启用 JTAG 调试等配置，还会将其中声明的镜像文件加载到内核地址，使 CPU 由 stand-by 状态开始执行内核初始化代码。

    在我开发的过程中参照 \href{https://github.com/rust-embedded/rust-raspberrypi-OS-tutorials}{rust-raspberrypi-OS-tutorials} 的串口传输工具完成了串口传输的配置。其中通过实现一个最小配置内核，实现了初始化对应端口（PIN 14,15）的替代方法声明以启动对应端口的传输声明。同时通过CH304 USB 转 TTL 串口传输模块发送开始传输信号给开发机中 Ruby 运行的应用程序，应用程序将内核镜像文件通过串口传输到树莓派4B内存中以完成镜像加载。最终最小配置内核将控权转交给内核镜像文件。

    \subsubsection{基于 Nix 构建可重构开发环境}

    作为一个标准开发操作系统的开发环境，必然是需要在同组内保持一定程度上的可重构性，易重构性以及同步性。基于这几种考量，我选用了 \href{https://nixos.org/}{Nix} flakes 对于项目整体依赖进行管理。就目前来看，除了对于使用到其他项目中的 docker 的部分，由于在 Non-NixOS 中，Nix 无法介入 systemctl 的管理而存在一定的不一致情况以及由于 WSL 对于串口设备连接的限制\footnote{在WSL中连接串口设备的时候，需要额外安装 usbipd}，其他的部分表现良好，均能很好的在 WSL, NixOS, Debian 等常用开发系统中构建一致，可用的开发环境。

    % 具体在实现过程中，我通过 flakes.inputs 固定了后面引用的 Nixpkgs, rust-overlays 库。
    % 同时，使用 overlays 在原先 nixpkgs 上掩盖了我自己的派生以保证开发环境构建的一致性。
    % 在代码段\ref{nix-flake-overlays} 第 7-10 行实现了对于 rust nightly toolchain 的固定，
    % 在后文 11-17 行实现了对于 nixpkgs 特定版本 qemu 的选择，在 18-21 行实现了对于联网获取的编译工具链的固定。
    
    \begin{lstlisting}[language=nix
        , caption=my flakes
        , label = {nix-flake-overlays}
        , numbers = left
        , breaklines=true
        , breakatwhitespace=true]
overlays = [ 
    (import rust-overlay)
    (self: super: {
        rust-toolchain =
        let rust = super.rust-bin; in
            # The rust-toolchain when i make this file, which maybe change
            (rust.nightly."2020-04-07".override {
            extensions = [ "rust-src" "llvm-tools-preview" "rustfmt" "clippy" ];
            targets = [ "x86_64-unknown-none" "riscv64gc-unknown-none-elf" "aarch64-unknown-none-softfloat" ];
            });
        qemu7 = self.callPackage "${nixpkgs-qemu7}/pkgs/applications/virtualization/qemu" {
        inherit (self.darwin.apple_sdk.frameworks) CoreServices Cocoa Hypervisor;
        inherit (self.darwin.stubs) rez setfile;
        inherit (self.darwin) sigtool;
        # Reduces the number of qemu source files from ~10000 to ~3619 source files.
        hostCpuTargets = ["riscv64-softmmu" "riscv32-softmmu" "x86_64-softmmu" "aarch64-softmmu" ];
        };
        x86_64-linux-musl-cross = fetchTarball {
        url = "https://musl.cc/x86_64-linux-musl-cross.tgz";
        sha256 = "172zrq1y4pbb2rpcw3swkvmi95bsqq1z6hfqvkyd9wrzv6rwm9jw";
        };
    })
    \end{lstlisting}

    同时，为了保证引入的工具链能完整的运行，我根据 \href{}{nixpkgs} 中提出的 issue，对于部分存在的问题进行了修复。

    \begin{lstlisting}[language=nix
        , caption=flakes 特殊适配
        , numbers = left
        , breaklines=true
        , breakatwhitespace=true]
unset OBJCOPY # Avoiding Overlay
export LIBCLANG_PATH="${pkgs.llvmPackages.libclang.lib}/lib" # nixpkgs@52447
export LD_LIBRARY_PATH="${pkgs.zlib}/lib:$LD_LIBRARY_PATH" # nixpkgs@92946

export PATH=$PATH:${pkgs.aarch64-linux-musl-cross}/bin: # ... etc
    \end{lstlisting}

%     \newpage

\subsection{ArceOS 操作系统现有驱动调用分析}

    下图\ref{fig::cvitek}左侧的部分是 ArceOS 操作系统的整体布局，右侧是现有\href{https://github.com/yuoo655/arceos_net/tree/hsp}{cvitek 物理网卡驱动} 的逐层调用情况。

    该 cvitek 物理网卡驱动主要作用在华山派，荔枝派等主机上。但与我们采用的树莓派4B中由 Soc 中集成 MAC 实现不一样，他们采用的这款设备提供了一个额外的以太网 MAC 控制器的 IP 核DWMAC 来完成 MAC 层的实现。

        
    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.4]{imgs/cvitek.jpg}
        \caption{cvitek 驱动调用栈}    \label{fig::cvitek}
    \end{figure}

    在整套实现中，ulib::libax 通过调用 module::axnet 所实现的底层方法（如TcpSocket，UdpSocket等）实现了用于 TCP/UDP 通信的通信源语。
    在 module::axnet 中这个部分的实现是针对于 
    \href{https://github.com/smoltcp-rs/smoltcp}{smoltcp} 这个 tcp/ip 协议栈进行了针对性的改造（非标准库环境等改造）而完成的。
    如果在编译的时候添加了对应的 features, ArceOS 会自动根据 module::axdriver::build.rs 文件中所进行的声明，
    将一个带有不同设备名称的 feature 加入默认 feature list 中，以方便实现基于设备组(phy, net, block, display) 实现的自动驱动加载。
    在完成 build.rs 编译脚本中的检查等操作之后，Cargo 在对于 axdriver 进行编译的时候，
    就会识别到当前编译携带了 cvitekphy/nic feature 从而根据 \#[cfg(feature = "cvitekphy")] 启用 cvitek\_traits.rs 的编译。
    cvitek\_traits.rs 文件将 ArceOS 上层 module 所提供给下层 crates 的方法支持（如dma\_alloc\_pages，delay等）
    通过 traits 的默认实现传递给下方的 crates 进行使用以实现 crates 层逆向调用 modules 层方法的效果。
    \footnote{在cvitek\_traits.rs 文件中 CvitekPhyTraits 声明并实现于 CvitekPhyTraitsImpl }。
    同时，在 module::axnet::driver.rs 中基于 cfg\_if 库的条件编译语句也实现了将 cvitek 网卡驱动转换成为 AxNetDevice 
    并实现了 Driver traits 下属的 probe\_global 方法的效果。
    最后在 module::axdriver 中 for\_each\_driver 宏的帮助下，ArceOS 将各个加载的网卡驱动转换成为 
    Driver 并运行（probe\_global）初始化，并将其添加到 AllDevices 下属的结构体中。

    而根据 ArceOS 的规定，cvitek 以太网卡驱动的实际实现实际上被封装在各自的 crates 中。在 crates::driver\_net 包规定了一系列网络设备、
    所必须要实现的 traits（如 transmit, receive）等方法。
    新的网络驱动会使用其内部方法实现这些对应的 traits，将这些方法包装成为 ArceOS 的调用方法。

\subsection{以太网卡驱动实现}