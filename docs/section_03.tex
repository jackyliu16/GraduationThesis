\section{系统开发流程}

    按照现有嵌入式企业的嵌入式软件开发流程，开发一个嵌入式系统主要分为以下几个步骤。

\subsection{用户需求分析}

    根据现有的各种企事业单位对于考勤打卡的需求，需求的特性主要为特征性识别以及日志记录。
    
    因此，在总体设计上我计划采用最简单的由指纹识别模块获取输入，经过 MCU 中简单处理再转发给
    linux 下的控制主机的设计。

\subsection{嵌入式开发环境的搭建与说明}

    由于嵌入式开发所基于的 MCU 一般性能相当有限，就算是采用一般 linux 操作系统进行本机编译，其占用时间也会相对比较长，同时，也无法应对一些占用系统内存资源较大的编译场景。
    因此，在嵌入式开发中，一般通过交叉编译的方式实现在 x86\_64-linux 平台或其他通用操作系统架构平台上实现对于目标平台代码的编译以更好的利用硬件资源。

    在本文的实现过程中基于 \href{https://nixos.org/}{Nix} 管理 x86\_64-linux 平台实现了对 aarch64-unknown-linux 目标平台的编译，其中主要的编译工具链的部分直接采用原有 \href{https://github.com/rcore-os/arceos}{ArceOS} 操作系统实现的 Rust 语言交叉编译以及镜像处理步骤，将 Cargo 包管理工具直接生成的裸机 elf 文件通过 rust-objcopy 去除其中一些无关的信息，如调试信息等内容成为一个纯粹的二进制文件。 
  
    一般树莓派的流程由 保存在 Soc ROM 区中的 booloader 完成 SD 卡上 FAT32 分区的挂载以及加载第二阶段 bootcode.bin，但是由于我使用的树莓派4B (bcm2711) 相对于前代有不少的硬件更新，在我使用的树莓派中对应初始化，启用 GPU fireware 加载 start.elf 的 bootloader 代码都被实现在 EEPROM 中，以提升 ROM 代码的容错性。 
    在运行 start4.elf 文件时，其会对 sd 卡中的 config.txt 文件进行解析，完成对应如串口传输频率，是否启用 JTAG 调试等配置，还会将其中声明的镜像文件加载到内核地址，使 CPU 由 stand-by 状态开始执行内核初始化代码。

    在我开发的过程中参照 \href{https://github.com/rust-embedded/rust-raspberrypi-OS-tutorials}{rust-raspberrypi-OS-tutorials} 的串口传输工具完成了串口传输的配置。其中通过实现一个最小配置内核，实现了初始化对应端口（PIN 14,15）的替代方法声明以启动对应端口的传输声明。同时通过CH304 USB 转 TTL 串口传输模块发送开始传输信号给开发机中 Ruby 运行的应用程序，应用程序将内核镜像文件通过串口传输到树莓派4B内存中以完成镜像加载。最终最小配置内核将控权转交给内核镜像文件。

    \subsubsection{基于 Nix 构建可重构开发环境}

    作为一个标准开发操作系统的开发环境，必然是需要在同组内保持一定程度上的可重构性，易重构性以及同步性。基于这几种考量，我选用了 \href{https://nixos.org/}{Nix} flakes 对于项目整体依赖进行管理。就目前来看，除了对于使用到其他项目中的 docker 的部分，由于在 Non-NixOS 中，Nix 无法介入 systemctl 的管理而存在一定的不一致情况以及由于 WSL 对于串口设备连接的限制\footnote{在WSL中连接串口设备的时候，需要额外安装 usbipd}，其他的部分表现良好，均能很好的在 WSL, NixOS, Debian 等常用开发系统中构建一致，可用的开发环境。

    % 具体在实现过程中，我通过 flakes.inputs 固定了后面引用的 Nixpkgs, rust-overlays 库。
    % 同时，使用 overlays 在原先 nixpkgs 上掩盖了我自己的派生以保证开发环境构建的一致性。
    % 在代码段\ref{nix-flake-overlays} 第 7-10 行实现了对于 rust nightly toolchain 的固定，
    % 在后文 11-17 行实现了对于 nixpkgs 特定版本 qemu 的选择，在 18-21 行实现了对于联网获取的编译工具链的固定。
    
    \begin{lstlisting}[language=nix
        , caption=my flakes
        , label = {nix-flake-overlays}
        , numbers = left
        , breaklines=true
        , breakatwhitespace=true]
overlays = [ 
    (import rust-overlay)
    (self: super: {
        rust-toolchain =
        let rust = super.rust-bin; in
            # The rust-toolchain when i make this file, which maybe change
            (rust.nightly."2020-04-07".override {
            extensions = [ "rust-src" "llvm-tools-preview" "rustfmt" "clippy" ];
            targets = [ "x86_64-unknown-none" "riscv64gc-unknown-none-elf" "aarch64-unknown-none-softfloat" ];
            });
        qemu7 = self.callPackage "${nixpkgs-qemu7}/pkgs/applications/virtualization/qemu" {
        inherit (self.darwin.apple_sdk.frameworks) CoreServices Cocoa Hypervisor;
        inherit (self.darwin.stubs) rez setfile;
        inherit (self.darwin) sigtool;
        # Reduces the number of qemu source files from ~10000 to ~3619 source files.
        hostCpuTargets = ["riscv64-softmmu" "riscv32-softmmu" "x86_64-softmmu" "aarch64-softmmu" ];
        };
        x86_64-linux-musl-cross = fetchTarball {
        url = "https://musl.cc/x86_64-linux-musl-cross.tgz";
        sha256 = "172zrq1y4pbb2rpcw3swkvmi95bsqq1z6hfqvkyd9wrzv6rwm9jw";
        };
    })
    \end{lstlisting}

    同时，为了保证引入的工具链能完整的运行，我根据 \href{}{nixpkgs} 中提出的 issue，对于部分存在的问题进行了修复。

    \begin{lstlisting}[language=nix
        , caption=flakes 特殊适配
        , numbers = left
        , breaklines=true
        , breakatwhitespace=true]
unset OBJCOPY # Avoiding Overlay
export LIBCLANG_PATH="${pkgs.llvmPackages.libclang.lib}/lib" # nixpkgs@52447
export LD_LIBRARY_PATH="${pkgs.zlib}/lib:$LD_LIBRARY_PATH" # nixpkgs@92946

export PATH=$PATH:${pkgs.aarch64-linux-musl-cross}/bin: # ... etc
    \end{lstlisting}

%     \newpage

\subsection{ArceOS 操作系统现有驱动调用分析}

    下图\ref{fig::cvitek}左侧的部分是 ArceOS 操作系统的整体布局，右侧是现有\href{https://github.com/yuoo655/arceos_net/tree/hsp}{cvitek 物理网卡驱动} 的逐层调用情况。

    该 cvitek 物理网卡驱动主要作用在华山派，荔枝派等主机上。但与我们采用的树莓派4B中由 Soc 中集成 MAC 实现不一样，他们采用的这款设备提供了一个额外的以太网 MAC 控制器的 IP 核DWMAC 来完成 MAC 层的实现。

        
    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.4]{imgs/cvitek.jpg}
        \caption{cvitek 驱动调用栈}    \label{fig::cvitek}
    \end{figure}

    在整套实现中，ulib::libax 通过调用 module::axnet 所实现的底层方法（如TcpSocket，UdpSocket等）实现了用于 TCP/UDP 通信的通信源语。
    在 module::axnet 中这个部分的实现是针对于 
    \href{https://github.com/smoltcp-rs/smoltcp}{smoltcp} 这个 tcp/ip 协议栈进行了针对性的改造（非标准库环境等改造）而完成的。
    如果在编译的时候添加了对应的 features, ArceOS 会自动根据 module::axdriver::build.rs 文件中所进行的声明，
    将一个带有不同设备名称的 feature 加入默认 feature list 中，以方便实现基于设备组(phy, net, block, display) 实现的自动驱动加载。
    在完成 build.rs 编译脚本中的检查等操作之后，Cargo 在对于 axdriver 进行编译的时候，
    就会识别到当前编译携带了 cvitekphy/nic feature 从而根据 \#[cfg(feature = "cvitekphy")] 启用 cvitek\_traits.rs 的编译。
    cvitek\_traits.rs 文件将 ArceOS 上层 module 所提供给下层 crates 的方法支持（如dma\_alloc\_pages，delay等）
    通过 traits 的默认实现传递给下方的 crates 进行使用以实现 crates 层逆向调用 modules 层方法的效果。
    \footnote{在cvitek\_traits.rs 文件中 CvitekPhyTraits 声明并实现于 CvitekPhyTraitsImpl }。
    同时，在 module::axnet::driver.rs 中基于 cfg\_if 库的条件编译语句也实现了将 cvitek 网卡驱动转换成为 AxNetDevice 
    并实现了 Driver traits 下属的 probe\_global 方法的效果。
    最后在 module::axdriver 中 for\_each\_driver 宏的帮助下，ArceOS 将各个加载的网卡驱动转换成为 
    Driver 并运行（probe\_global）初始化，并将其添加到 AllDevices 下属的结构体中。

    而根据 ArceOS 的规定，cvitek 以太网卡驱动的实际实现实际上被封装在各自的 crates 中。在 crates::driver\_net 包规定了一系列网络设备、
    所必须要实现的 traits（如 transmit, receive）等方法。
    新的网络驱动会使用其内部方法实现这些对应的 traits，将这些方法包装成为 ArceOS 的调用方法。

\subsection{以太网卡驱动实现}

    根据前文的分析，如果想要实现在 TRANSPORT 层或者 NETWORK 层实现树莓派和主机之间的通信效果，主要需要实现以太网 OSI 七层模型中的 DATA\_LINK 与 PHYDIVSL 层之间的通信，
    由下图\ref{fig::dataLink}可知，主要需要实现的部分在于使 Soc 上的 MAC 实现能通过 GMII, RGMII，Serial-GMII 等接口标准与 PHY 芯片进行联通，进而调用 PHY 芯片上对于以太网传输介质上光，电等信号进行解析的方法。
        
    \begin{figure}[ht]
        \centering
        \includegraphics[scale=0.4]{imgs/data_link_layers.jpg}
        \caption{IEEE 802.8 数据链路层}    \label{fig::dataLink}
    \end{figure}

    基于 树莓派 4B Soc bcm2711 芯片手册，其板载 PHY 芯片 BCM54213PE 数据表以及 IEEE 802.3 协议手册可以了解到，目前 Soc 和 PHY 之间是存在直连的 RGMII 通信接口的。
    在 RGMII 通信接口中由 GTK\_CLK, RX\_CLK 实现双向时钟同步，TXD[0:3], RXD[0:3] 实现数据传输，TX\_CTL，RX\_CTL 实现数据传输控制，MDIO 和 MDC 实现 MAC 与 物理层的控制和状态关系的设置。
    \footnote{虽然在bcm2711数据手册GPIO替代函数表中有提到提供了 RGMII\_MDIO 与 RGMII\_MDC Pins，但是没有找到任何公开资料描述这个部分与底层之间是如何联系起来的}

    % TODO: 其中 GPIO 中也提供了一组 MDIO MDC 有点没太明白这段是为什么添加到这个地方？

    \subsubsection{概念介绍}
    % NOTE: 我个人感觉没有必要在这里介绍一系列 MDIO 芯片等是如何初始化的，但是如果要凑字数的话感觉也还行 (

    \subsubsection{源码分析}

    根据对于 uboot 源码所进行的分析，在以太网 PHY 芯片中维持了如下的数据结构\ref{fig::uboot-genet-struct}。

    \begin{minipage}[t]{0.42\linewidth}
    \begin{lstlisting}[columns=fullflexible, label={fig::uboot-genet-struct}]       +
0x0    | IOBASE
+------|-----------------------------+
0x2000 | GENET_RX_OFF
       | [DMA_DESC_SIZE;TOTAL_DESCS]
0x2C00 | GENET_RDMA_REG_OFF
       | [DMA_RING_SIZE;DEFAULT_Q]
0x3000 | RDMA_RING_REG_BASE
       | [DMA_RING_SIZE]
0x3040 | RDMA_REG_BASE
       | dma_reg
+------|-----------------------------+
0x4000 | GENET_TX_OFF
       | As above ...
       +
        \end{lstlisting}
    \end{minipage}
    \quad
    \begin{minipage}[t]{0.5\linewidth}
        \vspace{2em}
        \setlength{\parindent}{1em}
        其中，IOBASE 为 BCM54213PE 芯片基于 MMIO 地址映射，在树莓派内存中对应的地址。
        在树莓派网卡中，0x2000-0x4000的地址主要分配给与Rx相关的结构体。0x4000-0x6000的地址分配给TX相关的结构体。

        这些结构体都是以太网卡中 DMA 模块至关重要的成分。
        以 0x2000 GENET\_RX\_OFF 开头，到 GENET\_RDMA\_REG\_OFF 为止的一段地址中保存了 256 个 DMA 描述符结构。
    \end{minipage}

    以 0x3000 GENET\_RDMA\_REG\_OFF 开头，到 RDMA\_RING\_REG\_BASE 为止的这一段地址中保存了 BCM54213PE 
    所支持的 16 个不同优先级别的接受环（方便 DMA 实现基于不同优先级的接受），
    还在 RDMA\_RING\_REG\_BASE 与 RDMA\_RING\_REG\_BASE 之间保存了一个默认的接受环。

    也就是说在 BCM54213PE 的硬件实现中提供了对于 16 个优先级队列，以及一个默认队列的支持。不过根据源代码，
    实际上不管是树莓派官方的 linux 内核或者 uboot 下的以太网驱动都没有全部使用这些队列。
    在树莓派中，将 256 个 DMA 描述符分配给五个队列，其中前四个队列分别占有 32 个描述符，第五个，也就是默认队列
    占有剩余的 128 个描述符。而在 uboot 中的实现考虑到了其需求，对于原先的设计进行了删减，
    直接将 256 个 DMA 描述符全部分给了默认队列（并没有使用优先级队列）。
    
    \begin{adjustwidth}{0cm}{} % NOTE: 消除全局段首锁紧带来的影响
    \begin{minipage}[b]{0.5\linewidth}
        \setlength{\parindent}{2em} % 段首缩进
        在 BCM54213PE DMA 模块中以太网模块初始化的过程中，会逐个将使用的 Rings 环进行初始化，
        每一个抽象意味上的环都是由 start\_addr 到 end\_addr 中的一段连续的 DMA 描述符组合而成的。
        每一个DMA描述符指向一段特定长度的内存空间，这也就是后文所提到的缓冲区的概念\ref{code::InitRxRings}。

    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\linewidth}
        \includegraphics[scale=0.6]{./imgs/Rings_and_Descs.jpg}        
        \captionof{figure}{DMA 描述符与环}
    \end{minipage}
    \end{adjustwidth}

    \newpage
    \begin{lstlisting}[language=C
        , caption={Initialize Rx priority queues}
        , label={code::InitRxRings}
        , numbers = left
        , breaklines=true
        , breakatwhitespace=true]
for (i = 0; i < priv->hw_params->rx_queues; i++) {
    ret = bcmgenet_init_rx_ring(priv, i,
                    priv->hw_params->rx_bds_per_q,
                    i * priv->hw_params->rx_bds_per_q,
                    (i + 1) *
                    priv->hw_params->rx_bds_per_q);
    if (ret)
        return ret;

    ring_cfg |= (1 << i);
    dma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));

static int bcmgenet_init_rx_ring(struct bcmgenet_priv *priv,
                unsigned int index, unsigned int size,
                unsigned int start_ptr, unsigned int end_ptr);
}
    \end{lstlisting}

    % \begin{figure}[htp]
    %     \flushleft
    %     \includegraphics[scale=0.4]{./imgs/Rings_and_Descs.jpg}        
    %     \caption{DMA 描述符与环}    
    %     % \label{fig::dataLink}
    % \end{figure}
    
    % % 也就是说在 BCM54213PE 的硬件实现中提供了对于 16 个优先级队列，以及一个默认队列的支持。不过根据源代码，
    % % 实际上不管是树莓派官方的 linux 内核或者 uboot 下的以太网驱动都没有全部使用这些队列。
    % % 在树莓派中，将 256 个 DMA 描述符分配给五个队列，其中前四个队列分别占有 32 个描述符，第五个，也就是默认队列
    % % 占有剩余的 128 个描述符。而在 uboot 中的实现考虑到了其需求，对于原先的设计进行了删减，
    % % 直接将 256 个 DMA 描述


    % % \begin{minipage}[t]{0.3\linewidth}
    % %     \includegraphics[scale=0.4]{./imgs/Rings_and_Descs.jpg}
    % %     \captionof{figure}{DMA 描述符与环}
    % % \end{minipage}
    % % \quad

    % % \begin{minipage}[t]{0.3\linewidth}
    % %     根据左侧的图以及 DMA 的相关说明，在 BCM54213PE 上 DMA 模块中，每一个环都相当于数据结构中的一个循环队列。
    % %     这个循环队列以
    % % \end{minipage}



